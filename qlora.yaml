model_name_or_path: "meta-llama/Meta-Llama-3.1-8B" # Hugging Face model id
# training parameters
learning_rate: 0.0002                  # learning rate 2e-4
lr_scheduler_type: "linear"          # learning rate scheduler
num_train_epochs: 5                    # number of training epochs
per_device_train_batch_size: 1         # batch size per device
per_device_eval_batch_size: 1          # batch size per device for evaluation
gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass
optim: adamw_torch                     # use torch adamw optimizer
logging_steps: 100                      # log every 10 steps
save_strategy: epoch                   # save checkpoint every epoch
evaluation_strategy: epoch             # evaluate every epoch
max_grad_norm: 0.3                     # max gradient norm
warmup_ratio: 0.03                     # warmup ratio
gradient_checkpointing: true           # use gradient checkpointing to save memory
seed: 42                               # random seed
#bf16: true                             # use bfloat16 precision
#tf32: false                             # use tf32 precision
# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp
#fsdp: "full_shard auto_wrap offload" # remove offload if enough GPU memory
#fsdp_config:
#  backward_prefetch: "backward_pre"
#  forward_prefetch: "false"
#  use_orig_params: "false"